{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the model\n",
    "\n",
    "This section imports the required packages and downloads the data used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing copy of required datasets from personal Github repository if not found in current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for data in  /home/peter/Documents/Hons/NLP/GPTiny/LMDatasets\n",
      "Data already downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download the ZIP file using wget\n",
    "ZIP_URL = 'https://github.com/tpchikumbu/GPTiny/archive/main.zip'\n",
    "PROJECT_DIR = os.getcwd() + '/LMDatasets'\n",
    "print('Searching for data in ', PROJECT_DIR)\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "\n",
    "  !wget -O \"GPTiny-main.zip\" \"$ZIP_URL\"\n",
    "  # Extract only the specific folder from the ZIP file\n",
    "  !unzip -q \"GPTiny-main.zip\" \"GPTiny-main/LMDatasets/*\" -d \".\"\n",
    "  !mv \"GPTiny-main/LMDatasets\" \".\"\n",
    "\n",
    "  # Remove temporary files\n",
    "  !rm -rf \"GPTiny-main.zip\"\n",
    "  !rm -rf \"GPTiny-main\"\n",
    "\n",
    "else:\n",
    "  print('Data already downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to choose a language for the datasets and read them into environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  6382803\n",
      "Dev shape:  441906\n",
      "Test shape:  444199\n"
     ]
    }
   ],
   "source": [
    "language = \"nr\" # Options include nr, ss, xh, zu\n",
    "file_name = f'{PROJECT_DIR}/nchlt_text.{language}'\n",
    "\n",
    "# Load data\n",
    "with open(file_name + \".train\", 'r', encoding='utf-8') as f:\n",
    "    train_df = f.read()\n",
    "with open(file_name + \".valid\", 'r', encoding='utf-8') as f:\n",
    "    dev_df = f.read()\n",
    "with open(file_name + \".test\", 'r', encoding='utf-8') as f:\n",
    "    test_df = f.read()\n",
    "\n",
    "print('Train shape: ', len(train_df))\n",
    "print('Dev shape: ', len(dev_df))\n",
    "print('Test shape: ', len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Vocabulary\n",
    "The next block must be run to generate a vocabulary and encode the datasets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  \n",
      " !\"$%&')*+,-./0123456789:;<>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_abcdefghijklmnopqrstuvwxyz{|}~ ¡£©«­°±²³´¸¹ºÂÃÅž\n",
      "Token count:  110\n"
     ]
    }
   ],
   "source": [
    "# Generate vocabulary\n",
    "used_chars = sorted(list(set(train_df)))\n",
    "vocab_size = len(used_chars)\n",
    "print(\"Tokens: \", ''.join(used_chars))\n",
    "print(\"Token count: \", vocab_size)\n",
    "\n",
    "# char to int mapping\n",
    "char_to_int = { ch:i for i,ch in enumerate(used_chars) }\n",
    "int_to_char = { i:ch for i,ch in enumerate(used_chars) }\n",
    "encode = lambda s: [char_to_int[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([int_to_char[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Tokenise the corpus and place on tensors\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Torch device: \", device)\n",
    "\n",
    "train_encoded = torch.tensor(encode(train_df), device=device)\n",
    "dev_encoded = torch.tensor(encode(dev_df), device=device)\n",
    "test_encoded = torch.tensor(encode(test_df), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network definition\n",
    "Contains the code required to define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self attention head\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head to find parallel attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, head_dropout=0.1, multi_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, head_dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(multi_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-forward network\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    widening : int = 4\n",
    "\n",
    "    def __init__(self, n_embd, wide = 4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.widening = wide\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, self.widening * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.widening * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute + communicate\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size, widen = 4, head_dropout=0.1, multi_dropout=0.1, ff_dropout=0.1, block_dropout=0.1):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, head_dropout, multi_dropout)\n",
    "        self.ffwd = FeedFoward(n_embd, widen, ff_dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.dropout = nn.Dropout(block_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # residual to attention block\n",
    "        x = x + self.ffwd(self.ln2(x)) # residual to feed forward block\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Hyperparameters\n",
    "The following block calculates average sentence length in the corpus. This value can be used for context size when training the model. Prioritise in sentence context, over cross sentence context. Other model hyperparameters like batch size, amount of dropout applied and connections between layers are also specified here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data on newline character and calculate average length of the split elements\n",
    "avg_length = 0\n",
    "sentence_count = 0\n",
    "for element in train_df.split('\\n'):\n",
    "  avg_length += len(element)\n",
    "  sentence_count += 1\n",
    "\n",
    "avg_length = round(avg_length / sentence_count)\n",
    "\n",
    "print(\"Total splits: \", sentence_count)\n",
    "print(\"Average length of split elements: \", avg_length)\n",
    "\n",
    "# ------------\n",
    "# Hyperparameters\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 256 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # avg_length # what is the maximum context length for predictions?\n",
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "learning_rate = 4e-2\n",
    "eval_iters = 100\n",
    "n_embd = 64\n",
    "n_head = 16\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other functions required during training can be defined to load the desired dataset and estimate the loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataloader with different block and batch sizes\n",
    "\n",
    "def get_batch(split: str):\n",
    "  # generate a small batch of data of inputs x and targets y\n",
    "\n",
    "  if split == \"train\":\n",
    "    data = train_encoded\n",
    "  elif split == \"dev\":\n",
    "    data = dev_encoded\n",
    "  elif split == \"test\":\n",
    "    data = test_encoded\n",
    "\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  return x, y\n",
    "\n",
    "# Estimate loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss(mode: str, model: nn.Module):\n",
    "    out = {}\n",
    "    splits = []\n",
    "    model.eval()\n",
    "    # Determine datasets to be used\n",
    "    if mode == \"train\":\n",
    "        splits = ['train', 'dev']\n",
    "    elif mode == \"test\":\n",
    "        splits = ['test']\n",
    "\n",
    "    # Calculate losses for chosen datasets\n",
    "    for split in splits:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
