{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the model\n",
    "\n",
    "Initial project setup includes importing the required packages and downloading the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Optional import for data visualization\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for data in  /home/peter/Documents/Hons/NLP/GPTiny/LMDatasets\n",
      "Data already downloaded\n"
     ]
    }
   ],
   "source": [
    "# Location of training data\n",
    "ZIP_URL = 'https://github.com/tpchikumbu/GPTiny/archive/main.zip'\n",
    "PROJECT_DIR = os.getcwd() + '/LMDatasets'\n",
    "\n",
    "# Check if the data is already downloaded\n",
    "print('Searching for data in ', PROJECT_DIR)\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  # Download the compressed project files\n",
    "  !wget -O \"GPTiny-main.zip\" \"$ZIP_URL\"\n",
    "\n",
    "  # Extract only the specific folder from the ZIP file\n",
    "  !unzip -q \"GPTiny-main.zip\" \"GPTiny-main/LMDatasets/*\" -d \".\"\n",
    "  !mv \"GPTiny-main/LMDatasets\" \".\"\n",
    "\n",
    "  # Remove temporary files\n",
    "  !rm -rf \"GPTiny-main.zip\"\n",
    "  !rm -rf \"GPTiny-main\"\n",
    "\n",
    "else:\n",
    "  print('Data already downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code selects which language from the dataset must be used for training. Available languages are:\n",
    "\n",
    "- isiXhosa (xh)\n",
    "- isiZulu (zu)\n",
    "- siSwati (ss)\n",
    "- isiNdebele (nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  6382803\n",
      "Dev shape:  441906\n",
      "Test shape:  444199\n"
     ]
    }
   ],
   "source": [
    "language = \"nr\" # Options are nr, ss, xh, zu\n",
    "file_name = f'{PROJECT_DIR}/nchlt_text.{language}'\n",
    "\n",
    "# Load data\n",
    "with open(file_name + \".train\", 'r', encoding='utf-8') as f:\n",
    "    train_df = f.read()\n",
    "with open(file_name + \".valid\", 'r', encoding='utf-8') as f:\n",
    "    dev_df = f.read()\n",
    "with open(file_name + \".test\", 'r', encoding='utf-8') as f:\n",
    "    test_df = f.read()\n",
    "\n",
    "print('Train shape: ', len(train_df))\n",
    "print('Dev shape: ', len(dev_df))\n",
    "print('Test shape: ', len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Vocabulary\n",
    "In natural language processing, a vocabulary refers to the collection of valid tokens within a dataset. The predictive nature of GPT prevents it from generating any new tokens.\n",
    "\n",
    "For ease of operation, these tokens are encoded as integers. The encoding used for this project was based on their order of appearance within the ASCII table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  \n",
      " !\"$%&')*+,-./0123456789:;<>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_abcdefghijklmnopqrstuvwxyz{|}~ ¡£©«­°±²³´¸¹ºÂÃÅž\n",
      "Token count:  110\n"
     ]
    }
   ],
   "source": [
    "# Generate vocabulary\n",
    "used_chars = sorted(list(set(train_df)))\n",
    "vocab_size = len(used_chars)\n",
    "print(\"Tokens: \", ''.join(used_chars))\n",
    "print(\"Token count: \", vocab_size)\n",
    "\n",
    "# char to int mapping\n",
    "char_to_int = { ch:i for i,ch in enumerate(used_chars) }\n",
    "int_to_char = { i:ch for i,ch in enumerate(used_chars) }\n",
    "encode = lambda s: [char_to_int[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([int_to_char[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Tokenise the corpus and place on tensors\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Torch device: \", device)\n",
    "\n",
    "train_encoded = torch.tensor(encode(train_df), device=device)\n",
    "dev_encoded = torch.tensor(encode(dev_df), device=device)\n",
    "test_encoded = torch.tensor(encode(test_df), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network definition\n",
    "\n",
    "Specifies the self-attention, multi-head attention and feed-forward modules contained in each decoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self attention head\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head to find parallel attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, head_dropout=0.1, multi_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, head_dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(multi_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-forward network\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    widening : int = 4\n",
    "\n",
    "    def __init__(self, n_embd, wide = 4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.widening = wide\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, self.widening * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.widening * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute + communicate\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size, widen = 4, head_dropout=0.1, multi_dropout=0.1, ff_dropout=0.1, block_dropout=0.1):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, head_dropout, multi_dropout)\n",
    "        self.ffwd = FeedFoward(n_embd, widen, ff_dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.dropout = nn.Dropout(block_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # residual to attention block\n",
    "        x = x + self.ffwd(self.ln2(x)) # residual to feed forward block\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Hyperparameters\n",
    "The following block calculates average sentence length in the corpus. This value can be used for context size when training the model. Prioritise in sentence context, over cross sentence context. Other model hyperparameters like batch size, amount of dropout applied and connections between layers are also specified here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total splits:  57099\n",
      "Average length of split elements:  111\n"
     ]
    }
   ],
   "source": [
    "# Split training data on newline character and calculate average length of the split elements\n",
    "avg_length = 0\n",
    "sentence_count = 0\n",
    "for element in train_df.split('\\n'):\n",
    "  avg_length += len(element)\n",
    "  sentence_count += 1\n",
    "\n",
    "avg_length = round(avg_length / sentence_count)\n",
    "\n",
    "print(\"Total splits: \", sentence_count)\n",
    "print(\"Average length of split elements: \", avg_length)\n",
    "\n",
    "# ------------\n",
    "# Hyperparameters\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 256 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # avg_length # what is the maximum context length for predictions?\n",
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "learning_rate = 4e-2\n",
    "eval_iters = 100\n",
    "n_embd = 64\n",
    "n_head = 16\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other functions required during training can be defined to load the desired dataset and estimate the loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataloader with different block and batch sizes\n",
    "\n",
    "def get_batch(split: str):\n",
    "  # generate a small batch of data of inputs x and targets y\n",
    "\n",
    "  if split == \"train\":\n",
    "    data = train_encoded\n",
    "  elif split == \"dev\":\n",
    "    data = dev_encoded\n",
    "  elif split == \"test\":\n",
    "    data = test_encoded\n",
    "\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  return x, y\n",
    "\n",
    "# Estimate loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss(mode: str, model: nn.Module):\n",
    "    out = {}\n",
    "    splits = []\n",
    "    model.eval()\n",
    "    # Determine datasets to be used\n",
    "    if mode == \"train\":\n",
    "        splits = ['train', 'dev']\n",
    "    elif mode == \"test\":\n",
    "        splits = ['test']\n",
    "\n",
    "    # Calculate losses for chosen datasets\n",
    "    for split in splits:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic bigram language model\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "This section starts with an optional bit of code to mount your Google drive when running this notebook on Colabs. This allows training outputs to be maintained after training completes. Alternatively, output will be placed into the 'GPTiny_losses' directory within the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Mount drive and create directory to store results\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# os.makedirs('/content/drive/MyDrive/GPTiny_loss', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output directory for results\n",
    "if 'google.colab' in sys.modules:\n",
    "  if os.path.isdir('/content/drive/MyDrive/GPTiny_loss'):\n",
    "    # Store on mounted drive if available\n",
    "    output = '/content/drive/MyDrive/GPTiny_loss/'\n",
    "#   else:\n",
    "#     os.makedirs('/content/GPTiny_losses', exist_ok=True)\n",
    "#     output = '/content/GPTiny_losses/'\n",
    "else:\n",
    "  os.makedirs('./GPTiny_losses', exist_ok=True)\n",
    "  output = './GPTiny_losses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output file for results\n",
    "param_str = f\"lr{learning_rate}_bth{batch_size}_blk{block_size}_drop{dropout}_hed{n_head}_lay{n_layer}\"\n",
    "loss_file = f\"{output}losses_{param_str}.csv\"\n",
    "\n",
    "if not os.path.isfile(loss_file):\n",
    "    with open(loss_file, 'w') as f:\n",
    "        # Add header to file\n",
    "        f.write(\"iter,train_loss,val_loss,BPC\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for specified iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model = LanguageModel().to(device)\n",
    "#m = model\n",
    "\n",
    "# create a PyTorch optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_encoded) // batch_size, epochs=max_iters)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Calculate validation loss after fixed interval\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(\"train\")\n",
    "        bpc = losses['dev'] / torch.log(torch.tensor(2.0))\n",
    "        print(f\"Epoch {iter}: | Train loss {losses['train']:.4f} | Validation loss {losses['dev']:.4f} | BPC: {bpc:.4f}\")\n",
    "\n",
    "        # Write the losses to a csv file\n",
    "        with open(loss_file, 'a') as f:\n",
    "            f.write(f\"{iter},{losses['train']},{losses['dev']},{bpc:.4f}\\n\")\n",
    "\n",
    "    # Sample batch of training data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Feed data to model to model and evaluate inputs\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# Calculate elapsed time in minutes\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print(f\"Training time: {elapsed_time} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate line plot of training and validation loss\n",
    "\n",
    "# Read the loss file into a pandas DataFrame\n",
    "df = pd.read_csv(loss_file, nrows = 1 + (max_iters // eval_iters))\n",
    "\n",
    "# Extract the iteration, training loss, and validation loss columns\n",
    "iters = df['iter']\n",
    "train_losses = df['train_loss']\n",
    "val_losses = df['val_loss']\n",
    "val_bpc = df['BPC']\n",
    "\n",
    "# Plot the training and validation losses\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the losses\n",
    "# ax1.axhline(y=min(val_losses), color='r', linestyle='--', label=\"Minimum\")\n",
    "ax1.plot(iters, train_losses, label='Training Loss')\n",
    "ax1.plot(iters, val_losses, label='Validation Loss')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot BPC\n",
    "ax2.plot(iters, val_bpc, label='BPC')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('BPC')\n",
    "ax2.set_title('Validation BPC')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
